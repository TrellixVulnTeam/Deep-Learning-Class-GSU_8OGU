{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T23:16:00.938278Z","iopub.execute_input":"2022-03-28T23:16:00.939074Z","iopub.status.idle":"2022-03-28T23:16:07.852885Z","shell.execute_reply.started":"2022-03-28T23:16:00.938934Z","shell.execute_reply":"2022-03-28T23:16:07.851957Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:07.854352Z","iopub.execute_input":"2022-03-28T23:16:07.854587Z","iopub.status.idle":"2022-03-28T23:16:08.610957Z","shell.execute_reply.started":"2022-03-28T23:16:07.854559Z","shell.execute_reply":"2022-03-28T23:16:08.610088Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport statistics","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:08.612497Z","iopub.execute_input":"2022-03-28T23:16:08.612890Z","iopub.status.idle":"2022-03-28T23:16:09.711312Z","shell.execute_reply.started":"2022-03-28T23:16:08.612859Z","shell.execute_reply":"2022-03-28T23:16:09.710434Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"directories = ['../input/csc4851-homework4/birds_400/test',\n                                '../input/csc4851-homework4/birds_400/train',\n                                '../input//csc4851-homework4/birds_400/valid']\n\nfor dir in directories:\n    label = []\n    path = []\n    for dirname, _,filenames in os.walk(dir):\n        for filename in filenames:\n            label.append(os.path.split(dirname)[1])\n            path.append(os.path.join(dirname,filename))\n    if dir == directories[0]:\n        df_test = pd.DataFrame(columns=['path','label'])\n        df_test['path']=path\n        df_test['label']=label\n    elif dir == directories[1]:\n        df_train = pd.DataFrame(columns=['path','label'])\n        df_train['path']=path\n        df_train['label']=label        \n    elif dir == directories[2]:\n        df_valid = pd.DataFrame(columns=['path','label'])\n        df_valid['path']=path\n        df_valid['label']=label\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:09.713302Z","iopub.execute_input":"2022-03-28T23:16:09.713580Z","iopub.status.idle":"2022-03-28T23:16:10.691738Z","shell.execute_reply.started":"2022-03-28T23:16:09.713550Z","shell.execute_reply":"2022-03-28T23:16:10.690921Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:10.693068Z","iopub.execute_input":"2022-03-28T23:16:10.694592Z","iopub.status.idle":"2022-03-28T23:16:10.711649Z","shell.execute_reply.started":"2022-03-28T23:16:10.694546Z","shell.execute_reply":"2022-03-28T23:16:10.710813Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\n\nfrom torchvision.datasets import ImageFolder\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom PIL import Image\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:10.713764Z","iopub.execute_input":"2022-03-28T23:16:10.714368Z","iopub.status.idle":"2022-03-28T23:16:12.020457Z","shell.execute_reply.started":"2022-03-28T23:16:10.714319Z","shell.execute_reply":"2022-03-28T23:16:12.019660Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Hyper Parameters \ninput_size = 4320000         # this is 224x224 I think this is mainly for logistic regression\nnum_classes = 400            # Length of bird in csv file\nnum_epochs = 200              # this is fine may need to increase it \nbatch_size = 256             # tune batch size according to CPU or the GPU\nlearning_rate = 0.001\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" ","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:12.021638Z","iopub.execute_input":"2022-03-28T23:16:12.021843Z","iopub.status.idle":"2022-03-28T23:16:12.027288Z","shell.execute_reply.started":"2022-03-28T23:16:12.021818Z","shell.execute_reply":"2022-03-28T23:16:12.026311Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:12.028898Z","iopub.execute_input":"2022-03-28T23:16:12.029330Z","iopub.status.idle":"2022-03-28T23:16:12.041728Z","shell.execute_reply.started":"2022-03-28T23:16:12.029299Z","shell.execute_reply":"2022-03-28T23:16:12.041091Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trans = transforms.Compose([                                           # Defining a variable transforms\n                    transforms.Resize((150,150)),                      # Resize the image to 256x256 pixels\n                    #transforms.CenterCrop(224),                       # Crop the image to 224x224 pixels about the center\n                    transforms.ToTensor(),                             # 0-255 to 0-1 numpy to tensor\n                    transforms.Normalize(mean=(0.5, 0.5, 0.5),         # Normalize the image\n                                        std= (0.5, 0.5, 0.5))           # Mean and std of image as also used when training\n                    ])","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:12.042987Z","iopub.execute_input":"2022-03-28T23:16:12.043521Z","iopub.status.idle":"2022-03-28T23:16:12.053125Z","shell.execute_reply.started":"2022-03-28T23:16:12.043485Z","shell.execute_reply":"2022-03-28T23:16:12.052516Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# PyTorch DataLoader\ndirectories = ['../input/csc4851-homework4/birds_400/test',\n                                '../input/csc4851-homework4/birds_400/train',\n                                '../input//csc4851-homework4/birds_400/valid']\ntrain_loader = DataLoader(ImageFolder(directories[0], transform=trans), \n                          batch_size=64, shuffle=True, #num_workers=3, pin_memory=True\n                        )\ntest_loader  = DataLoader(ImageFolder(directories[1], transform=trans), \n                          batch_size=32, shuffle=True, #num_workers=3, pin_memory=True\n                        )\nvalid_loader = DataLoader(ImageFolder(directories[2], transform=trans), \n                          batch_size, #num_workers=3, pin_memory=True\n                        )","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:12.055928Z","iopub.execute_input":"2022-03-28T23:16:12.056250Z","iopub.status.idle":"2022-03-28T23:16:13.207733Z","shell.execute_reply.started":"2022-03-28T23:16:12.056221Z","shell.execute_reply":"2022-03-28T23:16:13.206933Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ConvNet(nn.Module):\n    def __init__(self,num_classes=400):\n        super(ConvNet,self).__init__()\n        \n        #Output size after convolution filter\n        #((w-f+2P)/s) +1\n        \n        #Input shape= (256,3,150,150)\n        \n        self.conv1 = nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n        #Shape = (256,12,150,150)\n        self.bn1 = nn.BatchNorm2d(num_features=12)\n        #Shape= (256,12,150,150)\n        self.relu1 = nn.ReLU()\n        #Shape = (256,12,150,150)\n        \n        self.pool = nn.MaxPool2d(kernel_size=2)\n        #Reduce the image size be factor 2\n        #Shape = (256,12,75,75)\n        \n        \n        self.conv2 = nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,20,75,75)\n        self.relu2 = nn.ReLU()\n        #Shape = (256,20,75,75)\n        \n        \n        self.conv3 = nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,32,75,75)\n        self.bn3 = nn.BatchNorm2d(num_features=32)\n        #Shape= (256,32,75,75)\n        self.relu3 = nn.ReLU()\n        #Shape= (256,32,75,75)\n        \n        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n        \n        \n    #Feed forwad function    \n    def forward(self,inp):\n        output=self.conv1(inp)\n        output=self.bn1(output)\n        output=self.relu1(output)\n            \n        output=self.pool(output)\n            \n        output=self.conv2(output)\n        output=self.relu2(output)\n            \n        output=self.conv3(output)\n        output=self.bn3(output)\n        output=self.relu3(output)\n            \n            \n        #Above output will be in matrix form, with shape (256,32,75,75)\n            \n        output=output.view(-1,32*75*75)\n            \n        output=self.fc(output)\n            \n        return output\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:13.208816Z","iopub.execute_input":"2022-03-28T23:16:13.209175Z","iopub.status.idle":"2022-03-28T23:16:13.222585Z","shell.execute_reply.started":"2022-03-28T23:16:13.209147Z","shell.execute_reply":"2022-03-28T23:16:13.221629Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class AlexNet(torch.nn.Module):\n\n    def __init__(self, num_classes):\n        super().__init__()\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n            torch.nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n            torch.nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = torch.nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(256 * 6 * 6, 4096),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(4096, 4096),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), 256 * 6 * 6)\n        logits = self.classifier(x)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:13.223771Z","iopub.execute_input":"2022-03-28T23:16:13.224029Z","iopub.status.idle":"2022-03-28T23:16:13.240911Z","shell.execute_reply.started":"2022-03-28T23:16:13.224000Z","shell.execute_reply":"2022-03-28T23:16:13.239972Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pathlib\n\nroot = pathlib.Path(directories[0])\nclasses = sorted(j.name.split('/')[-1] for j in root.iterdir())             \n\n#model = ConvNet(num_classes).to(device)\nmodel = AlexNet(num_classes).to(device)\n\n# Optimizer and loss function\n# Parameters have been set as global values\nloss_function = nn.CrossEntropyLoss()  \n#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)  \noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n\ntrain_count = len(glob.glob(directories[0]+'/**/*.jpg'))\ntest_count = len(glob.glob(directories[1]+'/**/*.jpg'))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:13.242268Z","iopub.execute_input":"2022-03-28T23:16:13.242499Z","iopub.status.idle":"2022-03-28T23:16:14.463215Z","shell.execute_reply.started":"2022-03-28T23:16:13.242472Z","shell.execute_reply":"2022-03-28T23:16:14.462427Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(classes)\nprint(train_count)\nprint(test_count)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:14.464323Z","iopub.execute_input":"2022-03-28T23:16:14.464576Z","iopub.status.idle":"2022-03-28T23:16:14.469831Z","shell.execute_reply.started":"2022-03-28T23:16:14.464548Z","shell.execute_reply":"2022-03-28T23:16:14.469002Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train(model, loss_fn, optimizer):\n    \n    #set the module in training mode\n    model.train()\n    \n    train_batch_losses = []\n    \n    for batch, labels in train_loader:\n        \n        #send the training data to the GPU\n        batch = batch.to(device)\n        labels = labels.to(device)\n        \n        #set all gradients to zero\n        optimizer.zero_grad()\n        \n        #forward propagate\n        y_pred = model(batch)\n        \n        #calculate the loss\n        loss = loss_fn(y_pred, labels)\n        \n        #bachpropagate\n        loss.backward()\n        \n        #update the parameters (weights and biases)\n        optimizer.step()\n        \n        train_batch_losses.append(float(loss))\n        \n    mean_loss = statistics.mean(train_batch_losses)\n        \n    return mean_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:14.471446Z","iopub.execute_input":"2022-03-28T23:16:14.471976Z","iopub.status.idle":"2022-03-28T23:16:14.482326Z","shell.execute_reply.started":"2022-03-28T23:16:14.471935Z","shell.execute_reply":"2022-03-28T23:16:14.481620Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def validate(model, loss_fn, optimizer):\n    \n    # set the model in evaluation mode\n    model.eval()\n    \n    # save predictions for later\n    pedrictions = []\n    \n    # stop tracking the parameters for backpropagation\n    with torch.no_grad():\n        \n        validation_batch_losses = []\n        \n        for batch, labels in valid_loader:\n            \n            # send the validation data to GPU\n            batch = batch.to(device)\n            labels = labels.to(device)\n            \n            # forward propagate\n            labels_pred = model(batch)\n            \n            # calculate loss\n            loss = loss_fn(labels_pred, labels)\n            \n            validation_batch_losses.append(float(loss))\n            \n            mean_loss = statistics.mean(validation_batch_losses)\n           \n    return mean_loss ","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:14.483530Z","iopub.execute_input":"2022-03-28T23:16:14.483762Z","iopub.status.idle":"2022-03-28T23:16:14.496821Z","shell.execute_reply.started":"2022-03-28T23:16:14.483733Z","shell.execute_reply":"2022-03-28T23:16:14.495888Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def accuracy(model, loader):\n    correct = 0\n    total = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for batch, labels in loader:\n            batch = batch.to(device)\n            labels = labels.to(device)\n            \n            labels_pred = model(batch)\n            \n            _, predicted = torch.max(labels_pred.data, 1)\n        \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            return (100 * correct / total)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:14.498322Z","iopub.execute_input":"2022-03-28T23:16:14.498678Z","iopub.status.idle":"2022-03-28T23:16:14.513838Z","shell.execute_reply.started":"2022-03-28T23:16:14.498622Z","shell.execute_reply":"2022-03-28T23:16:14.512893Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def training_stats(train_loss, train_accuracy, val_loss, val_accuracy):\n    print(('training loss: {:.3f} '\n           'training accuracy: {:.2f}% || '\n           'val. loss: {:.3f} '\n           'val. accuracy: {:.2f}%').format(train_loss, train_accuracy,\n                                            val_loss, val_accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:14.515012Z","iopub.execute_input":"2022-03-28T23:16:14.515233Z","iopub.status.idle":"2022-03-28T23:16:14.525740Z","shell.execute_reply.started":"2022-03-28T23:16:14.515205Z","shell.execute_reply":"2022-03-28T23:16:14.524838Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\n\ntrain_losses = []\nvalid_losses = []\n\nfor epoch in range(1, 1+10):\n    \n    print('Epoch number', epoch)\n    \n    train_loss = train(model, loss_fn, optimizer)\n    train_losses.append(train_loss)\n    train_accuracy = accuracy(model, train_loader)\n    \n    valid_loss = validate(model, loss_fn, optimizer)\n    valid_losses.append(valid_loss)\n    valid_accuracy = accuracy(model, valid_loader)\n\n    training_stats(train_loss, train_accuracy, valid_loss, valid_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:16:14.526849Z","iopub.execute_input":"2022-03-28T23:16:14.527150Z","iopub.status.idle":"2022-03-28T23:20:21.457491Z","shell.execute_reply.started":"2022-03-28T23:16:14.527107Z","shell.execute_reply":"2022-03-28T23:20:21.456130Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation,BatchNormalization, Flatten, Conv2D, MaxPooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras import losses\nfrom keras.preprocessing.image import ImageDataGenerator\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport os\nfrom sklearn.metrics import log_loss\nfrom glob import glob\nimport csv  ","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:44:49.566113Z","iopub.execute_input":"2022-03-28T23:44:49.566406Z","iopub.status.idle":"2022-03-28T23:44:49.573325Z","shell.execute_reply.started":"2022-03-28T23:44:49.566376Z","shell.execute_reply":"2022-03-28T23:44:49.572657Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Parsing all the t\npath = '/kaggle/input/csc4851-homework4/birds_400/train/'\ntrain_datagen = ImageDataGenerator(rescale=1. / 224)\ntrain = train_datagen.flow_from_directory(path, target_size=(224,224), class_mode='categorical')\n\nclasses = [x[0].split('/')[-1] for x in os.walk(path)]\nclasses.pop(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:44:56.029888Z","iopub.execute_input":"2022-03-28T23:44:56.030182Z","iopub.status.idle":"2022-03-28T23:44:57.836092Z","shell.execute_reply.started":"2022-03-28T23:44:56.030149Z","shell.execute_reply":"2022-03-28T23:44:57.835244Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Using alexnet\ndef AlexNet(input_shape):\n    \n    X_input = Input(input_shape)\n    \n    X = Conv2D(96,(11,11),strides = 4,name=\"conv0\")(X_input)\n    X = BatchNormalization(axis = 3 , name = \"bn0\")(X)\n    X = Activation('relu')(X)\n    \n    X = MaxPooling2D((3,3),strides = 2,name = 'max0')(X)\n    \n    X = Conv2D(256,(5,5),padding = 'same' , name = 'conv1')(X)\n    X = BatchNormalization(axis = 3 ,name='bn1')(X)\n    X = Activation('relu')(X)\n    \n    X = MaxPooling2D((3,3),strides = 2,name = 'max1')(X)\n    \n    X = Conv2D(384, (3,3) , padding = 'same' , name='conv2')(X)\n    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(384, (3,3) , padding = 'same' , name='conv3')(X)\n    X = BatchNormalization(axis = 3, name = 'bn3')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(256, (3,3) , padding = 'same' , name='conv4')(X)\n    X = BatchNormalization(axis = 3, name = 'bn4')(X)\n    X = Activation('relu')(X)\n    \n    X = MaxPooling2D((3,3),strides = 2,name = 'max2')(X)\n    \n    X = Flatten()(X)\n    \n    X = Dense(4096, activation = 'relu', name = \"fc0\")(X)\n    \n    X = Dense(4096, activation = 'relu', name = 'fc1')(X) \n    \n    X = Dense(400,activation='softmax',name = 'fc2')(X)\n    model = Model(inputs = X_input, outputs = X, name='AlexNet')\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:45:01.665663Z","iopub.execute_input":"2022-03-28T23:45:01.665973Z","iopub.status.idle":"2022-03-28T23:45:01.679325Z","shell.execute_reply.started":"2022-03-28T23:45:01.665941Z","shell.execute_reply":"2022-03-28T23:45:01.678318Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"alex = AlexNet(train[0][0].shape[1:])","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:45:07.022043Z","iopub.execute_input":"2022-03-28T23:45:07.023019Z","iopub.status.idle":"2022-03-28T23:45:07.543211Z","shell.execute_reply.started":"2022-03-28T23:45:07.022963Z","shell.execute_reply":"2022-03-28T23:45:07.542368Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"alex.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:45:09.701133Z","iopub.execute_input":"2022-03-28T23:45:09.701730Z","iopub.status.idle":"2022-03-28T23:45:09.719755Z","shell.execute_reply.started":"2022-03-28T23:45:09.701684Z","shell.execute_reply":"2022-03-28T23:45:09.717072Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"alex.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:45:12.948507Z","iopub.execute_input":"2022-03-28T23:45:12.949354Z","iopub.status.idle":"2022-03-28T23:45:12.958750Z","shell.execute_reply.started":"2022-03-28T23:45:12.949306Z","shell.execute_reply":"2022-03-28T23:45:12.957775Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"alex.fit_generator(train,epochs=7)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:45:15.044158Z","iopub.execute_input":"2022-03-28T23:45:15.044471Z","iopub.status.idle":"2022-03-28T23:45:27.548827Z","shell.execute_reply.started":"2022-03-28T23:45:15.044429Z","shell.execute_reply":"2022-03-28T23:45:27.547967Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"path_test = '/kaggle/input/csc4851-homework4/birds_400/test'\ntest_datagen = ImageDataGenerator(rescale=1. / 224)\ntest = test_datagen.flow_from_directory(path_test, target_size=(224,224), class_mode='categorical')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = alex.evaluate_generator(test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validation\n\npath_test = '/kaggle/input/csc4851-homework4/birds_400/valid'\nvalidation_datagen = ImageDataGenerator(rescale=1. / 224)\nvalidation = validation_datagen.flow_from_directory(path_test, target_size=(224,224), batch_size = 1,class_mode='categorical')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare(img_path):\n    img = image.load_img(img_path, target_size=(224,224))\n    x = image.img_to_array(img)\n    x = x/224\n    return np.expand_dims(x, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cce = losses.CategoricalCrossentropy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = alex.predict([prepare(\"/kaggle/input/csc4851-homework4/birds_400/test/ABBOTTS BABBLER/1.jpg\")])\n# print(len(result[0]))\ntemplate = [0]*400\nlog_loss = {}\ntest_images = glob(\"/kaggle/input/csc4851-homework4/birds_400/test/\" + \"*/*.jpg\")\n\nfor path in test_images:\n    result = alex.predict([prepare(path)])\n    actual_class = path.split('/')[-2]\n    actual_class = \"BLACK & YELLOW  BROADBILL\" if actual_class == \"BLACK & YELLOW BROADBILL\" else actual_class\n    actual_index = classes.index(actual_class)\n    template[actual_index] = 1\n    log_loss_current = cce(template, result[0]).numpy()\n#     print(log_loss_current)\n    if actual_index in log_loss:\n        log_loss[actual_index] += (log_loss_current)/100\n    else:\n        log_loss[actual_index] = (log_loss_current)/100\n    template[actual_index] = 0\n\n# print(template)\n# print(classes[int(x)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Generating submission.csv file...')\nids = list(log_loss.keys())\nvalues = list(log_loss.values())\n\nf = open('submission.csv', 'w')\nwriter = csv.writer(f)\nwriter.writerow(['id','birds'])\nfor index in range(len(ids)):\n    writer.writerow([ids[index],values[index]])\n\n!head submission.csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display 20 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\ndf_sample = df_train.sample(15)\ndf_sample.reset_index(drop=True, inplace=True)\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df_sample.path[i]))\n    ax.set_title(df_sample.label[i])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:20:21.461624Z","iopub.status.idle":"2022-03-28T23:20:21.462092Z","shell.execute_reply.started":"2022-03-28T23:20:21.461840Z","shell.execute_reply":"2022-03-28T23:20:21.461866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}